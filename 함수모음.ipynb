{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 박스플랏\n",
    "def boxplot(df,ncols,nrows): \n",
    "    plt.style.use('seaborn')\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.rcParams[\"font.family\"] = \"Malgun Gothic\"\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, constrained_layout=True)\n",
    "\n",
    "    fig.set_size_inches((20, 100))\n",
    "\n",
    "    for col, ax in zip(df.columns, axs.T.ravel()):\n",
    "        df[[col]].boxplot(ax=ax)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distplot\n",
    "def dist (df,col_n,row_n):\n",
    "    \n",
    "    plt.rcParams['font.family'] = \"Malgun Gothic\"\n",
    "    plt.rcParams['axes.unicode_minus'] = False # 마이너스숫자 출력처리\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=col_n, nrows=row_n, figsize=(20,row_n*5))\n",
    "\n",
    "    for i, col in enumerate(df.columns[:]):\n",
    "        sns.distplot(df[col], bins=20, ax=ax[int(i/col_n),int(i%col_n)] , color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qqplot(df,ncols,nrows):\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.rcParams[\"font.family\"] = \"Malgun Gothic\"\n",
    "   \n",
    "    fig, axs = plt.subplots(nrows, ncols, constrained_layout=True)\n",
    "    fig.set_size_inches((50, 80))\n",
    "\n",
    "    for ax, i in zip(axs.ravel(), df):\n",
    "            stats.probplot(df[i], dist=stats.norm, plot=ax)\n",
    "            ax.set_title(str(i))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.light_palette(\"darkgray\", as_cmap = True)\n",
    "sns.set(font=\"Malgun Gothic\",rc = {'figure.figsize':(16,8)})  \n",
    "\n",
    "#pairplot\n",
    "sns.pairplot(x,cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 히트맵\n",
    "colormap = plt.cm.PuBu\n",
    "plt.figure(figsize=(100, 100))\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rc('font', family=\"Malgun Gothic\")\n",
    "plt.title(\"Correlation of Features\", y = 1.05, size = 15)\n",
    "sns.heatmap(x_train_stan.astype(float).corr(), linewidths = 0.1, ,vmax = 1.0, \n",
    "            vmin= -1.0 , square = True, cmap = colormap, linecolor = \"white\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 샤피로 테스트\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def Shapiro(df):\n",
    "    Shapi = []\n",
    "    Shapi.append([col for col in df])\n",
    "    for i in Shapi:\n",
    "        for j in i:\n",
    "            shapiro_test,p_val = shapiro(df[j])\n",
    "            print(j,\"Test-statistics : {}, p-value : {}\". format(shapiro_test,p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 엔더슨 테스트\n",
    "from scipy import stats\n",
    "from scipy.stats import anderson\n",
    "\n",
    "def Anderson(df):\n",
    "    ander = []\n",
    "    ander.append([col for col in df])\n",
    "    for i in ander:\n",
    "        for j in i:\n",
    "            anderson_test = anderson(df[j], dist='norm')\n",
    "            print(j,anderson_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규성 검정을 하나의 함수로\n",
    "from scipy.stats import shapiro, anderson, kstest, jarque_bera, normaltest\n",
    "\n",
    "def normal_test(test_name,x):\n",
    "    normal = []\n",
    "    notnormal = []\n",
    "    if test_name == 'shapiro':\n",
    "        for var in x.columns :\n",
    "            stat, p  = shapiro(x[var].values)\n",
    "            alpha = 0.05\n",
    "            if p > alpha :\n",
    "                normal.append(var)\n",
    "            else :\n",
    "                notnormal.append(var)\n",
    "                \n",
    "    elif test_name == 'anderson':\n",
    "        for var in x.columns :\n",
    "            result  = anderson(x[var].values,dist='norm')\n",
    "            normality = 0\n",
    "            for i in range(len(result.critical_values)):\n",
    "                # sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "                if result.statistic < result.critical_values[i]:\n",
    "                    normality +=1\n",
    "                else :\n",
    "                    normality +=0\n",
    "            if normality > 2.5 :\n",
    "                normal.append(var)\n",
    "            else :\n",
    "                notnormal.append(var)\n",
    "                \n",
    "    elif test_name == 'kstest':\n",
    "        for var in x.columns :\n",
    "            stat,p  = kstest(x[var].values, \"norm\")\n",
    "            alpha = 0.05\n",
    "            if p > alpha :\n",
    "                normal.append(var)\n",
    "            else :\n",
    "                notnormal.append(var)\n",
    "    \n",
    "    elif test_name == 'jarque_bera':\n",
    "        for var in x.columns :\n",
    "            stat,p  = jarque_bera(x[var].values)\n",
    "            alpha = 0.05\n",
    "            if p > alpha :\n",
    "                normal.append(var)\n",
    "            else :\n",
    "                notnormal.append(var)\n",
    "                \n",
    "    elif test_name == 'normaltest':\n",
    "        for var in x.columns :\n",
    "            stat, p  = normaltest(x[var].values)\n",
    "            alpha = 0.05\n",
    "            if p > alpha :\n",
    "                normal.append(var)\n",
    "            else :\n",
    "                notnormal.append(var)\n",
    "    else:\n",
    "        pass\n",
    "    return pd.DataFrame(normal, columns = ['normal']),pd.DataFrame(notnormal, columns = ['notnormal'])\n",
    "# 1) shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H가 1인 경우 이분산성 / H가 0인 경우 등분산\n",
    "from scipy.stats import bartlett\n",
    "import pandas as pd\n",
    "def bartlett_test(df, col, p_value = 0.05, H = 1):\n",
    "    list= []\n",
    "    for i in col:\n",
    "        T, p_val =bartlett(df[df['target']==1][i], df[df['target']==0][i]) \n",
    "        list.append([i, p_val])\n",
    "\n",
    "    list = pd.DataFrame(list, columns = ['변수', 'p_value'])\n",
    "    if H == 1:\n",
    "        a = list[(list['p_value'] < p_value)][['변수', 'p_value']].sort_values('p_value')\n",
    "        return a\n",
    "    else:\n",
    "        a = list[(list['p_value'] >= p_value)][['변수', 'p_value']].sort_values('p_value')\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# VIF 출력을 위한 데이터 프레임 형성\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# VIF 값과 각 Feature 이름에 대해 설정\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X_train_유의.values, i) for i in range(X_train_유의.shape[1])]\n",
    "vif[\"features\"] = X_train_유의.columns \n",
    "\n",
    "# VIF 값이 높은 순으로 정렬\n",
    "vif = vif.sort_values(by=\"VIF Factor\", ascending=False)\n",
    "vif = vif.reset_index().drop(columns='index')\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_feature_selection(X_train, y_train, variables=X_train.columns.tolist() ):\n",
    "    import statsmodels.api as sm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    y = y_train ## 반응 변수\n",
    "\n",
    "    selected_variables = [] ## 선택된 변수들\n",
    "    sl_enter = 0.05\n",
    "    sl_remove = 0.05\n",
    "    \n",
    "    sv_per_step = [] ## 각 스텝별로 선택된 변수들\n",
    "    adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수\n",
    "    steps = [] ## 스텝\n",
    "    step = 0\n",
    "    while len(variables) > 0:\n",
    "        remainder = list(set(variables) - set(selected_variables))\n",
    "        pval = pd.Series(index=remainder) ## 변수의 p-value\n",
    "        ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n",
    "        ## 선형 모형을 적합한다.\n",
    "        for col in remainder: \n",
    "            X = X_train[selected_variables+[col]]\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(y,X).fit(disp=0)\n",
    "            pval[col] = model.pvalues[col]\n",
    "    \n",
    "        min_pval = pval.min()\n",
    "        if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n",
    "            selected_variables.append(pval.idxmin())\n",
    "            ## 선택된 변수들에대해서\n",
    "            ## 어떤 변수를 제거할지 고른다.\n",
    "            while len(selected_variables) > 0:\n",
    "                selected_X = X_train[selected_variables]\n",
    "                selected_X = sm.add_constant(selected_X)\n",
    "                selected_pval = sm.OLS(y,selected_X).fit(disp=0).pvalues[1:] ## 절편항의 p-value는 뺀다\n",
    "                max_pval = selected_pval.max()\n",
    "                if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n",
    "                    remove_variable = selected_pval.idxmax()\n",
    "                    selected_variables.remove(remove_variable)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            step += 1\n",
    "            steps.append(step)\n",
    "            adj_r_squared = sm.OLS(y,sm.add_constant(X_train[selected_variables])).fit(disp=0).rsquared_adj\n",
    "            adjusted_r_squared.append(adj_r_squared)\n",
    "            sv_per_step.append(selected_variables.copy())\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    fig = plt.figure(figsize=(100,10))\n",
    "    fig.set_facecolor('white')\n",
    "    \n",
    "    font_size = 15\n",
    "    plt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\n",
    "    plt.plot(steps,adjusted_r_squared, marker='o')\n",
    "      \n",
    "    plt.ylabel('Adjusted R Squared',fontsize=font_size)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IQR 이상치\n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "  # target 값과 상관관계가 높은 열을 우선적으로 진행\n",
    "  quantile_25 = np.percentile(df[column].values, 25)\n",
    "  quantile_75 = np.percentile(df[column].values, 75)\n",
    "\n",
    "  IQR = quantile_75 - quantile_25\n",
    "  IQR_weight = IQR*weight\n",
    "  \n",
    "  lowest = quantile_25 - IQR_weight\n",
    "  highest = quantile_75 + IQR_weight\n",
    "  \n",
    "  outlier_idx = df[column][ (df[column] < lowest) | (df[column] > highest) ].index\n",
    "  return outlier_idx\n",
    "\n",
    "# oulier_idx = get_outlier(df_수치, col, 1.5)\n",
    "# df_수치.drop(oulier_idx , axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESD이상치\n",
    "def EDS_outlier(df,col,weight=2):\n",
    "    import numpy as np\n",
    "    m = np.mean(df[col])\n",
    "    sd = np.std(df[col])\n",
    "    sd_weight = sd*weight\n",
    "    \n",
    "    lowest = m -sd_weight\n",
    "    highest = m + sd_weight\n",
    "\n",
    "    outlier_idx = df[col][ (df[col] < lowest) | (df[col] > highest) ].index\n",
    "    return outlier_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(test,pred):\n",
    "    acc = accuracy_score(test,pred)\n",
    "    f1 = f1_score(test,pred)\n",
    "    precision = precision_score(test,pred)\n",
    "    recall = recall_score(test,pred)\n",
    "    print('##############\\n',confusion_matrix(test,pred),\n",
    "    \"############\\n\",f'acc_score: {acc}\\n f1_score: {f1} \\n precision: {precision} \\n recall: {recall}')\n",
    "\n",
    "## 임계값 =[] 리스트생성 후 for 구문\n",
    "def get_eval_by_threshold(y_test , pred_proba_c1, thresholds=[0.1,0.71,0.72,0.73,0.74,0.75]):\n",
    "    # thresholds list객체내의 값을 차례로 iteration하면서 Evaluation 수행.\n",
    "    for custom_threshold in thresholds:\n",
    "        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) \n",
    "        custom_predict = binarizer.transform(pred_proba_c1)\n",
    "        print('임곗값:',custom_threshold)\n",
    "        eval(y_test , custom_predict)\n",
    "\n",
    "\n",
    "def precision_recall_curve_plot(y_test , pred_proba_c1):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "## roc curve_plot    \n",
    "def roc_curve_plot(y_test,pred_proba):\n",
    "    fprs, tprs, thresholds = roc_curve(y_test,pred_proba) ## 입력시 1레이블 컬럼만 추출\n",
    "    \n",
    "    # Roc curve를 plot 곡선으로 기름\n",
    "    plt.plot(fprs,tprs,label='ROC')\n",
    "    ## 가운데 대각선 직선을 그림\n",
    "    plt.plot([0,1],[0,1],\"k--\",label=\"ramdom\")\n",
    "    \n",
    "    ## fpr x축을 scale을 0.1단위로 변경 x,y축 명 설정\n",
    "    start,end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start,end,0.1),2))\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('FPR(1-sensitivity)')\n",
    "    plt.ylabel('TPR(recall)')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix 시각화를 위한 함수입니다.\n",
    "def plot_confusion_matrix(cm, y_true, y_pred, classes, normalize=False, cmap=plt.cm.OrRd):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from elice_utils import EliceUtils                    \n",
    "    title = \"\"\n",
    "    if normalize:\n",
    "        title = 'Normalized confusion matrix'\n",
    "    else:\n",
    "        title = 'Confusion matrix'\n",
    "    \n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        # 정규화 할 때는 모든 값을 더해서 합이 1이 되도록 각 데이터를 스케일링 합니다.\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(title, \":\\n\", cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # label을 45도 회전해서 보여주도록 변경\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # confusion matrix 실제 값 뿌리기\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig('confusion matrix.png')\n",
    "    elice_utils.send_image('confusion matrix.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
